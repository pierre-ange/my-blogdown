---
title: 'Classification: Bayes and k-NN algorithms'
author: "Pierre-Ange Oliva"
date: '2018-08-18'
slug: bayes-knn
comments: false
categories: [R]
tags: [Machine learning, R, Bayes, KNN]
large_thumb: true
draft: false
output: 
  blogdown::html_page:
    dev: 'png'
    highlight: tango
    css: "../../css/custom.css"
---

<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
</style>
  <link rel="stylesheet" href="../../css/custom.css" type="text/css" />


<p><a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf">The Elements of Statistical Learning</a> is an excellent book for anyone wanting to get a good grasp of some of the most frequently used machine learning algorithms. I started reading it a while ago now, and at the time there was one specific illustration in the early chapters of the book that I thought Iâ€™d like to come back to and reproduce later on.</p>
<p>This is the part where the classification setting is introduced, Chapter 2. It is mentionned that the Bayes classifier is the one classifier that produces the lowest error rate: <span class="math display">\[ P(Y=j|X=x_0) \]</span></p>
<p>For real data whose underlying distribution is unknown, the Bayes classifier cannot be used and approaches exist which try to get as close to it as possible. k-NN algorithm is one of them, and the authors illustrated the point by simulating data and drawing both the k-NN and Bayes classification boundaries (Figures 2.2 and 2.5 in the book):</p>
<center>
<img src="/images/esl-knn.jpeg" /> <img src="/images/esl-bayes.jpeg" />
</center>
<div id="lets-reproduce-it" class="section level2">
<h2>Letâ€™s reproduce it</h2>
<p>The first step is to generate the data ourselves - fortunately the authors described how they did it so it is not too difficult. Note that the data and the overall look of our plots wonâ€™t be the same as in the book because all of this is random, obviously ðŸ˜„.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(hrbrthemes)
<span class="kw">library</span>(MASS)
<span class="kw">library</span>(mvtnorm) <span class="co">#multivariate gaussian density</span>

<span class="kw">set.seed</span>(<span class="dv">5</span>)

<span class="co"># Function to draw n bivariate gaussian variables</span>
biv_gaussian &lt;-<span class="st"> </span><span class="cf">function</span>(mu, sigma, n){
  m &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">mvrnorm</span>(n, <span class="dt">mu =</span> mu, <span class="dt">Sigma =</span> sigma)
  <span class="kw">colnames</span>(m) &lt;-<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;x&quot;</span>, <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(mu))
  <span class="kw">as_tibble</span>(m)
}

<span class="co"># For each class 1 and 2, create a mixture of 10 low-variance Gaussian distributions, </span>
<span class="co"># with individual means themselves distributed as Gaussian.</span>
<span class="co"># This will be the training dataset for KNN learning algorithm</span>
mu &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">0</span>))
train &lt;-<span class="st"> </span>mu <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="co"># Draw 10 variables from gaussian distributions N(mu_i, 1)</span>
<span class="st">  </span>purrr<span class="op">::</span><span class="kw">map_df</span>(biv_gaussian, <span class="kw">diag</span>(<span class="dv">2</span>), <span class="dv">10</span>, <span class="dt">.id =</span> <span class="st">&quot;class&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>dplyr<span class="op">::</span><span class="kw">group_by</span>(class, <span class="dt">mean_x1 =</span> x1, <span class="dt">mean_x2 =</span> x2) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co"># Again, draw 10 variables from gaussian distributions centered around the previous points</span>
<span class="st">  </span>dplyr<span class="op">::</span><span class="kw">do</span>(<span class="kw">biv_gaussian</span>(<span class="kw">c</span>(.<span class="op">$</span>mean_x1,.<span class="op">$</span>mean_x2), <span class="kw">diag</span>(<span class="dv">2</span>)<span class="op">/</span><span class="dv">5</span>, <span class="dv">10</span>))

<span class="co"># Display these points</span>
x_max &lt;-<span class="st"> </span><span class="kw">ceiling</span>(<span class="kw">max</span>(<span class="kw">abs</span>(train[,<span class="op">-</span><span class="dv">1</span>]))) <span class="co"># Maximum absolute value of x coordinates</span>
<span class="kw">ggplot</span>(<span class="dt">data =</span> train, <span class="kw">aes</span>(<span class="dt">color =</span> class)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(x1, x2), <span class="dt">size =</span> .<span class="dv">8</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_colour_manual</span>(<span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;1&quot;</span> =<span class="st"> &quot;red&quot;</span>, <span class="st">&quot;2&quot;</span> =<span class="st"> &quot;blue&quot;</span>)) <span class="op">+</span>
<span class="st">  </span>hrbrthemes<span class="op">::</span><span class="kw">theme_ipsum</span>(<span class="dt">base_size =</span> <span class="dv">8</span>, <span class="dt">plot_title_size =</span> <span class="dv">12</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">coord_fixed</span>(<span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span>x_max,x_max), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span>x_max,x_max)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Training set&quot;</span>)</code></pre></div>
<p><img src="/post/2018-08-18-bayes-knn_files/figure-html/unnamed-chunk-1-1.png" width="70%" style="display: block; margin: auto;" /></p>
<div id="knn" class="section level3">
<h3>kNN</h3>
<p>Now letâ€™s train a 10 nearest neighbours algorithm on this dataset to predict whether a point would belong to class 1 or class 2.</p>
<p>A grid covering a squared area will be used as an evaluation set, whose nodes are coloured according to which class (1 or 2) the algorithm assigns them to. We also draw a path to highlight the boundary between classes 1 and 2. This will help us visualize how the kNN algorithm works.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Set up a mesh which will then be used as a testing set for the KNN algorithm</span>
n_steps &lt;-<span class="st"> </span><span class="dv">100</span>
steps &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span>x_max, x_max, <span class="dt">length.out =</span> n_steps)
test &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(<span class="kw">expand.grid</span>(<span class="dt">x1 =</span> steps, <span class="dt">x2 =</span> steps))

<span class="co">#---- kNN Model</span>
n_knn &lt;-<span class="st"> </span><span class="dv">10</span> <span class="co"># Parameters: number of neighbours</span>
predict &lt;-<span class="st"> </span>class<span class="op">::</span><span class="kw">knn</span>(train[,<span class="op">-</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>)], test, train<span class="op">$</span>class, n_knn, <span class="dt">prob =</span> <span class="ot">TRUE</span>)
prob &lt;-<span class="st"> </span><span class="kw">attr</span>(predict, <span class="st">&quot;prob&quot;</span>)
prob_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">ifelse</span>(predict <span class="op">==</span><span class="st"> &quot;1&quot;</span>, prob, <span class="dv">1</span><span class="op">-</span>prob) <span class="co"># Probability of class 1</span>
knn &lt;-<span class="st"> </span><span class="kw">mutate</span>(test, <span class="dt">Predict =</span> predict, <span class="dt">Prob =</span> prob) 

<span class="co"># kNN boundary between class1 and 2</span>
prob_matrix &lt;-<span class="st"> </span><span class="kw">matrix</span>(prob_<span class="dv">1</span>, n_steps, n_steps)
knn_boundary &lt;-<span class="st"> </span><span class="kw">contourLines</span>(steps, steps, prob_matrix, <span class="dt">levels =</span> <span class="fl">0.5</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>purrr<span class="op">::</span><span class="kw">map_df</span>(as.tibble, <span class="dt">.id =</span> <span class="st">&quot;Path&quot;</span>)

<span class="co"># Display the results</span>
<span class="kw">ggplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> train, <span class="kw">aes</span>(x1, x2, <span class="dt">color =</span> class), <span class="dt">size =</span> .<span class="dv">8</span>) <span class="op">+</span>
<span class="st">  </span><span class="co"># KNN</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> knn, <span class="kw">aes</span>(x1, x2, <span class="dt">color =</span> Predict), <span class="dt">size =</span> <span class="fl">0.01</span>, <span class="dt">alpha =</span> <span class="fl">0.3</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_path</span>(<span class="dt">data =</span> knn_boundary, <span class="kw">aes</span>(x, y, <span class="dt">group =</span> Path), <span class="dt">size =</span> .<span class="dv">5</span>) <span class="op">+</span>
<span class="st">  </span>hrbrthemes<span class="op">::</span><span class="kw">theme_ipsum</span>(<span class="dt">base_size =</span> <span class="dv">8</span>, <span class="dt">plot_title_size =</span> <span class="dv">12</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">panel.grid.major =</span> <span class="kw">element_blank</span>(),
        <span class="dt">panel.grid.minor =</span> <span class="kw">element_blank</span>(),
        <span class="dt">axis.title =</span> <span class="kw">element_blank</span>()) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_colour_manual</span>(<span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;1&quot;</span> =<span class="st"> &quot;red&quot;</span>, <span class="st">&quot;2&quot;</span> =<span class="st"> &quot;blue&quot;</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">coord_fixed</span>(<span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span>x_max,x_max), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span>x_max,x_max)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="kw">paste0</span>(<span class="st">&quot;KNN - &quot;</span>, n_knn))</code></pre></div>
<p><img src="/post/2018-08-18-bayes-knn_files/figure-html/unnamed-chunk-2-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We can see how most points of the training set are correctly assigned to their class, although not all points obviously, especially in the center area where it is less clear whether they belong to class 1 or 2.</p>
<p>Actually we can calculate the training error rate: 77.5% of the training set was correctly classified by the kNN algorithm.</p>
</div>
<div id="bayes" class="section level3">
<h3>Bayes</h3>
<p>Now letâ€™s use the same dataset and draw the Bayes boundary. It is possible to do so using:</p>
<p><span class="math display">\[ \begin{aligned}
P(Y = i| X) &amp;= \frac{P(X|Y=i)P(Y=i)}{P(X)} \\
&amp;= \frac{P(X|Y=i)P(Y=i)}{P(X|Y=1)P(Y=1) + P(X|Y=0)P(Y=0)}\\
&amp;= \frac{P(Y=1)}{P(Y=1)}\frac{P(X|Y=i)}{P(X|Y=1) + P(X|Y=0)}\\
&amp;= \frac{P(X|Y=i)}{P(X|Y=1) + P(X|Y=0)}\\
\end{aligned}
\]</span></p>
<p>It is possible to calculate each of these conditional probabilities, and so given a point we know how to obtain its probability of belonging to class 1 or 2. Considering now <span class="math display">\[ \frac{P(X|Y=1)}{P(X|Y=0)} \]</span> we can contour level 1 of this ratio and obtain the Bayes decision boundary!</p>
<p>Converting into code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#---- Bayes</span>
<span class="co"># Create a tibble of the 10 center coordinates, for each class</span>
means &lt;-<span class="st"> </span>train <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">distinct</span>(class, mean_x1, mean_x2) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">split</span>(.<span class="op">$</span>class) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">bind_cols</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="dt">mean_x1_1 =</span> mean_x1, <span class="dt">mean_x2_1 =</span> mean_x2,
                <span class="dt">mean_x1_2 =</span> mean_x11, <span class="dt">mean_x2_2 =</span> mean_x21)

bayes &lt;-<span class="st"> </span>test <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by_all</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Means =</span> <span class="kw">list</span>(means)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">unnest</span>(Means) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">rowwise</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">density_1 =</span> mvtnorm<span class="op">::</span><span class="kw">dmvnorm</span>(<span class="dt">x =</span> <span class="kw">c</span>(x1,x2), <span class="dt">mean =</span> <span class="kw">c</span>(mean_x1_<span class="dv">1</span>,mean_x2_<span class="dv">1</span>), <span class="dt">sigma =</span> <span class="kw">diag</span>(<span class="dv">2</span>)<span class="op">/</span><span class="dv">5</span>)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">density_2 =</span> mvtnorm<span class="op">::</span><span class="kw">dmvnorm</span>(<span class="dt">x =</span> <span class="kw">c</span>(x1,x2), <span class="dt">mean =</span> <span class="kw">c</span>(mean_x1_<span class="dv">2</span>,mean_x2_<span class="dv">2</span>), <span class="dt">sigma =</span> <span class="kw">diag</span>(<span class="dv">2</span>)<span class="op">/</span><span class="dv">5</span>)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(x1, x2) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarise_at</span>(<span class="kw">vars</span>(density_<span class="dv">1</span>, density_<span class="dv">2</span>), sum) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">d_ratio =</span> density_<span class="dv">1</span><span class="op">/</span>density_<span class="dv">2</span>)

prob_matrix &lt;-<span class="st"> </span><span class="kw">matrix</span>(bayes<span class="op">$</span>d_ratio, n_steps, n_steps, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)
bayes_boundary &lt;-<span class="st"> </span><span class="kw">contourLines</span>(steps, steps, prob_matrix, <span class="dt">levels =</span> <span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>purrr<span class="op">::</span><span class="kw">map_df</span>(as.tibble, <span class="dt">.id =</span> <span class="st">&quot;Path&quot;</span>)</code></pre></div>
<p>Here is our Bayes classification boundary:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> train, <span class="kw">aes</span>(x1, x2, <span class="dt">color =</span> class), <span class="dt">size =</span> .<span class="dv">8</span>) <span class="op">+</span>
<span class="st">  </span><span class="co"># Bayes Boundary</span>
<span class="st">  </span><span class="kw">geom_path</span>(<span class="dt">data =</span> bayes_boundary, <span class="kw">aes</span>(x, y, <span class="dt">group =</span> Path), <span class="dt">size =</span> .<span class="dv">5</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> bayes, <span class="kw">aes</span>(x1, x2, <span class="dt">colour =</span> d_ratio<span class="op">&gt;</span><span class="dv">1</span>), <span class="dt">size =</span> <span class="fl">0.01</span>, <span class="dt">alpha =</span> <span class="fl">0.3</span>) <span class="op">+</span>
<span class="st">  </span>hrbrthemes<span class="op">::</span><span class="kw">theme_ipsum</span>(<span class="dt">base_size =</span> <span class="dv">8</span>, <span class="dt">plot_title_size =</span> <span class="dv">12</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">panel.grid.major =</span> <span class="kw">element_blank</span>(),
        <span class="dt">panel.grid.minor =</span> <span class="kw">element_blank</span>(),
        <span class="dt">axis.title =</span> <span class="kw">element_blank</span>()) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_colour_manual</span>(<span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;1&quot;</span> =<span class="st"> &quot;red&quot;</span>, <span class="st">&quot;2&quot;</span> =<span class="st"> &quot;blue&quot;</span>, <span class="st">&quot;TRUE&quot;</span> =<span class="st"> &quot;red&quot;</span>, <span class="st">&quot;FALSE&quot;</span> =<span class="st"> &quot;blue&quot;</span>), <span class="dt">guide =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">coord_fixed</span>(<span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span>x_max,x_max), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span>x_max,x_max)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Bayes Boundary&quot;</span>)</code></pre></div>
<p><img src="/post/2018-08-18-bayes-knn_files/figure-html/unnamed-chunk-4-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>And now to finish the job, drawing both the kNN and Bayes decision boundaries:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> train, <span class="kw">aes</span>(x1, x2, <span class="dt">color =</span> class), <span class="dt">size =</span> .<span class="dv">8</span>) <span class="op">+</span>
<span class="st">  </span><span class="co"># Bayes Boundary</span>
<span class="st">  </span><span class="kw">geom_path</span>(<span class="dt">data =</span> bayes_boundary, <span class="kw">aes</span>(x, y, <span class="dt">group =</span> Path), <span class="dt">size =</span> .<span class="dv">5</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> bayes, <span class="kw">aes</span>(x1, x2, <span class="dt">colour =</span> d_ratio<span class="op">&gt;</span><span class="dv">1</span>), <span class="dt">size =</span> <span class="fl">0.01</span>, <span class="dt">alpha =</span> <span class="fl">0.3</span>) <span class="op">+</span>
<span class="st">  </span><span class="co"># KNN</span>
<span class="st">  </span><span class="kw">geom_path</span>(<span class="dt">data =</span> knn_boundary, <span class="kw">aes</span>(x, y, <span class="dt">group =</span> Path), <span class="dt">size =</span> .<span class="dv">5</span>, <span class="dt">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="op">+</span>
<span class="st">  </span>hrbrthemes<span class="op">::</span><span class="kw">theme_ipsum</span>(<span class="dt">base_size =</span> <span class="dv">8</span>, <span class="dt">plot_title_size =</span> <span class="dv">12</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">panel.grid.major =</span> <span class="kw">element_blank</span>(),
        <span class="dt">panel.grid.minor =</span> <span class="kw">element_blank</span>(),
        <span class="dt">axis.title =</span> <span class="kw">element_blank</span>()) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_colour_manual</span>(<span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;1&quot;</span> =<span class="st"> &quot;red&quot;</span>, <span class="st">&quot;2&quot;</span> =<span class="st"> &quot;blue&quot;</span>, <span class="st">&quot;TRUE&quot;</span> =<span class="st"> &quot;red&quot;</span>, <span class="st">&quot;FALSE&quot;</span> =<span class="st"> &quot;blue&quot;</span>), <span class="dt">guide =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">coord_fixed</span>(<span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span>x_max,x_max), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span>x_max,x_max)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Bayes and 10-NN Classifiers&quot;</span>)</code></pre></div>
<p><img src="/post/2018-08-18-bayes-knn_files/figure-html/unnamed-chunk-5-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
</div>
