---
title: 'Classification: Bayes and k-NN algorithms'
author: "Pierre-Ange Oliva"
date: '2018-08-18'
slug: bayes-knn
comments: false
categories: [R]
tags: [Machine learning, R, Bayes, KNN]
large_thumb: true
draft: false
output: 
  blogdown::html_page:
    dev: 'png'
    highlight: tango
    css: "../../css/custom.css"
    mathjax: true
---

```{r setup, include=F}
knitr::opts_chunk$set(message = FALSE, 
                      warning = FALSE, 
                      comment = FALSE, 
                      cache = TRUE, 
                      echo = TRUE, 
                      dpi = 300,
                      out.width = "70%",
                      fig.width = 8,
                      fig.height = 6,
                      fig.align = 'center')
```

[The Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) is an excellent book for anyone wanting to get a good grasp of some of the most frequently used machine learning algorithms. I started reading it a while ago now, and at the time there was one specific illustration in the early chapters of the book that I thought I'd like to come back to and reproduce later on. 

This is the part where the classification setting is introduced, Chapter 2. It is mentionned that the Bayes classifier is the one classifier that produces the lowest error rate: $$ P(Y=j|X=x_0) $$

For real data whose underlying distribution is unknown, the Bayes classifier cannot be used and approaches exist which try to get as close to it as possible. k-NN algorithm is one of them, and the authors illustrated the point by simulating data and drawing both the k-NN and Bayes classification boundaries (Figures 2.2 and 2.5 in the book):

<center>
![](/images/esl-knn.jpeg) ![](/images/esl-bayes.jpeg)
</center>

## Let's reproduce it

The first step is to generate the data ourselves - fortunately the authors described how they did it so it is not too difficult. Note that the data and the overall look of our plots won't be the same as in the book because all of this is random, obviously `r emo::ji("smile")`.

```{r}
library(tidyverse)
library(hrbrthemes)
library(MASS)
library(mvtnorm) #multivariate gaussian density

set.seed(5)

# Function to draw n bivariate gaussian variables
biv_gaussian <- function(mu, sigma, n){
  m <- MASS::mvrnorm(n, mu = mu, Sigma = sigma)
  colnames(m) <- paste0("x", 1:length(mu))
  as_tibble(m)
}

# For each class 1 and 2, create a mixture of 10 low-variance Gaussian distributions, 
# with individual means themselves distributed as Gaussian.
# This will be the training dataset for KNN learning algorithm
mu <- list(c(0,1), c(1,0))
train <- mu %>% 
  # Draw 10 variables from gaussian distributions N(mu_i, 1)
  purrr::map_df(biv_gaussian, diag(2), 10, .id = "class") %>% 
  dplyr::group_by(class, mean_x1 = x1, mean_x2 = x2) %>%
  # Again, draw 10 variables from gaussian distributions centered around the previous points
  dplyr::do(biv_gaussian(c(.$mean_x1,.$mean_x2), diag(2)/5, 10))

# Display these points
x_max <- ceiling(max(abs(train[,-1]))) # Maximum absolute value of x coordinates
ggplot(data = train, aes(color = class)) +
  geom_point(aes(x1, x2), size = .8) +
  scale_colour_manual(values = c("1" = "red", "2" = "blue")) +
  hrbrthemes::theme_ipsum(base_size = 8, plot_title_size = 12) +
  coord_fixed(xlim = c(-x_max,x_max), ylim = c(-x_max,x_max)) +
  labs(title = "Training set")
```


### kNN
Now let's train a 10 nearest neighbours algorithm on this dataset to predict whether a point would belong to class 1 or class 2. 

A grid covering a squared area will be used as an evaluation set, whose nodes are coloured according to which class (1 or 2) the algorithm assigns them to. We also draw a path to highlight the boundary between classes 1 and 2. This will help us visualize how the kNN algorithm works.

```{r}
# Set up a mesh which will then be used as a testing set for the KNN algorithm
n_steps <- 100
steps <- seq(-x_max, x_max, length.out = n_steps)
test <- as_tibble(expand.grid(x1 = steps, x2 = steps))

#---- kNN Model
n_knn <- 10 # Parameters: number of neighbours
predict <- class::knn(train[,-(1:3)], test, train$class, n_knn, prob = TRUE)
prob <- attr(predict, "prob")
prob_1 <- ifelse(predict == "1", prob, 1-prob) # Probability of class 1
knn <- mutate(test, Predict = predict, Prob = prob) 

# kNN boundary between class1 and 2
prob_matrix <- matrix(prob_1, n_steps, n_steps)
knn_boundary <- contourLines(steps, steps, prob_matrix, levels = 0.5) %>% 
  purrr::map_df(as.tibble, .id = "Path")

# Display the results
ggplot() +
  geom_point(data = train, aes(x1, x2, color = class), size = .8) +
  # KNN
  geom_point(data = knn, aes(x1, x2, color = Predict), size = 0.01, alpha = 0.3) +
  geom_path(data = knn_boundary, aes(x, y, group = Path), size = .5) +
  hrbrthemes::theme_ipsum(base_size = 8, plot_title_size = 12) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.title = element_blank()) +
  scale_colour_manual(values = c("1" = "red", "2" = "blue")) +
  coord_fixed(xlim = c(-x_max,x_max), ylim = c(-x_max,x_max)) +
  labs(title = paste0("KNN - ", n_knn))

```

We can see how most points of the training set are correctly assigned to their class, although not all points obviously, especially in the center area where it is less clear whether they belong to class 1 or 2.

Actually we can calculate the training error rate: `r sum(class::knn(train[,-(1:3)], train[,-(1:3)], train$class, n_knn) == train$class)/nrow(train)*100`% of the training set was correctly classified by the kNN algorithm.


### Bayes
Now let's use the same dataset and draw the Bayes boundary. It is possible to do so using:

$$ \begin{aligned}
P(Y = i| X) &= \frac{P(X|Y=i)P(Y=i)}{P(X)} \\
&= \frac{P(X|Y=i)P(Y=i)}{P(X|Y=1)P(Y=1) + P(X|Y=0)P(Y=0)}\\
&= \frac{P(Y=1)}{P(Y=1)}\frac{P(X|Y=i)}{P(X|Y=1) + P(X|Y=0)}\\
&= \frac{P(X|Y=i)}{P(X|Y=1) + P(X|Y=0)}\\
\end{aligned}
$$

It is possible to calculate each of these conditional probabilities, and so given a point we know how to obtain its probability of belonging to class 1 or 2.
Considering now $$ \frac{P(Y=1|X)}{P(Y=0|X)} $$ we can contour level 1 of this ratio and obtain the Bayes decision boundary!

Converting into code:

```{r}
#---- Bayes
# Create a tibble of the 10 center coordinates, for each class
means <- train %>% 
  distinct(class, mean_x1, mean_x2) %>% 
  split(.$class) %>% 
  bind_cols() %>% 
  ungroup() %>% 
  dplyr::select(mean_x1_1 = mean_x1, mean_x2_1 = mean_x2,
                mean_x1_2 = mean_x11, mean_x2_2 = mean_x21)

bayes <- test %>% 
  group_by_all() %>% 
  mutate(Means = list(means)) %>% 
  unnest(Means) %>% 
  rowwise() %>% 
  mutate(density_1 = mvtnorm::dmvnorm(x = c(x1,x2), mean = c(mean_x1_1,mean_x2_1), sigma = diag(2)/5)) %>% 
  mutate(density_2 = mvtnorm::dmvnorm(x = c(x1,x2), mean = c(mean_x1_2,mean_x2_2), sigma = diag(2)/5)) %>% 
  group_by(x1, x2) %>% 
  summarise_at(vars(density_1, density_2), sum) %>% 
  mutate(d_ratio = density_1/density_2)

prob_matrix <- matrix(bayes$d_ratio, n_steps, n_steps, byrow = TRUE)
bayes_boundary <- contourLines(steps, steps, prob_matrix, levels = 1) %>% 
  purrr::map_df(as.tibble, .id = "Path")
```

Here is our Bayes classification boundary:

```{r}
ggplot() +
  geom_point(data = train, aes(x1, x2, color = class), size = .8) +
  # Bayes Boundary
  geom_path(data = bayes_boundary, aes(x, y, group = Path), size = .5) +
  geom_point(data = bayes, aes(x1, x2, colour = d_ratio>1), size = 0.01, alpha = 0.3) +
  hrbrthemes::theme_ipsum(base_size = 8, plot_title_size = 12) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.title = element_blank()) +
  scale_colour_manual(values = c("1" = "red", "2" = "blue", "TRUE" = "red", "FALSE" = "blue"), guide = FALSE) +
  coord_fixed(xlim = c(-x_max,x_max), ylim = c(-x_max,x_max)) +
  labs(title = "Bayes Boundary")
```

And now to finish the job, drawing both the kNN and Bayes decision boundaries:

```{r}
ggplot() +
  geom_point(data = train, aes(x1, x2, color = class), size = .8) +
  # Bayes Boundary
  geom_path(data = bayes_boundary, aes(x, y, group = Path), size = .5) +
  geom_point(data = bayes, aes(x1, x2, colour = d_ratio>1), size = 0.01, alpha = 0.3) +
  # KNN
  geom_path(data = knn_boundary, aes(x, y, group = Path), size = .5, linetype = "dashed") +
  hrbrthemes::theme_ipsum(base_size = 8, plot_title_size = 12) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.title = element_blank()) +
  scale_colour_manual(values = c("1" = "red", "2" = "blue", "TRUE" = "red", "FALSE" = "blue"), guide = FALSE) +
  coord_fixed(xlim = c(-x_max,x_max), ylim = c(-x_max,x_max)) +
  labs(title = "Bayes and 10-NN Classifiers")
```

